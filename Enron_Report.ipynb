{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identify Fraud from Enron Email"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Machine learning techniques were applied to data from 146 Enron employees.  There were 21 features for each employee and 3066 total data points.  The data for each employee fell into 3 categories: financial, email, and poi label.  The poi label signified whether or not an employee was a person of interest in the Enron case.  18 employees were labeled as a poi and 128 employees were not.  5 machine learning algorithms were deployed to predict an employee's poi label based on their financial and email features.  1 algorithm was selected and tuned for final analysis.</p>\n",
    "\n",
    "### Data Structure\n",
    "\n",
    "The data was stored in a dictionary of dictionaries.  For the outer dict, each key was a person and each value was a feature name.  For the inner dict, each key was a feature name and each value was the value of that feature.  \n",
    "\n",
    "```Python\n",
    "datadict['SKILLING JEFFREY K']['salary']\n",
    "```\n",
    "\n",
    "###### Financial Features\n",
    "\n",
    "<p>['salary', 'deferral_payments', 'total_payments', 'loan_advances', 'bonus', 'restricted_stock_deferred', 'deferred_income', 'total_stock_value', 'expenses', 'exercised_stock_options', 'other', 'long_term_incentive', 'restricted_stock', 'director_fees'] (all units are in US dollars)</p>\n",
    "\n",
    "###### Email Features\n",
    "\n",
    "<p>['to_messages', 'email_address', 'from_poi_to_this_person', 'from_messages', 'from_this_person_to_poi', 'shared_receipt_with_poi'] (units are generally number of emails messages; notable exception is ‘email_address’, which is a text string)</p>\n",
    "\n",
    "###### POI Label\n",
    "\n",
    "<p>[‘poi’] (boolean, represented as integer)</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Missing Values\n",
    "\n",
    "<p>These are the total 'NaN' values for each feature.</p>\n",
    "\n",
    "POI: 0  \n",
    "Salary: 50  \n",
    "Deferred Income: 96  \n",
    "Loan Advances: 141  \n",
    "Other: 53  \n",
    "Long Term Incentive: 79  \n",
    "Percent Exercised Stock: 44  \n",
    "Percent Restricted Stock: 38  \n",
    "Percent Restricted Stock Deferred: 130  \n",
    "Percent to POI: 58  \n",
    "Percent from POI: 58  \n",
    "Percent Shared with POI: 58  \n",
    "Percent Deferral Payments: 106  \n",
    "Percent Expenses: 50  \n",
    "Percent Director Fees: 130  \n",
    "Percent Bonus: 63  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outliers\n",
    "\n",
    "<p>Many outliers in this dataset were important because they helped identify persons of interest.  However, some outliers did not correspond to a person.  The max value for financial features was from a 'TOTAL' key rather than a 'PERSON' key.  Another non-person key 'THE TRAVEL AGENCY IN THE PARK' was identified and these keys were removed from the data dictionary.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Feature Selection\n",
    "\n",
    "<p>SelectKBest was used to get the feature importances for all features.  The feature importances are based on the chi squared value between each feature and the poi label.</p>\n",
    "\n",
    "exercised_stock_options: 24.815079733218194  \n",
    "total_stock_value: 24.182898678566879  \n",
    "bonus: 20.792252047181535  \n",
    "salary: 18.289684043404513  \n",
    "deferred_income: 11.458476579280369  \n",
    "long_term_incentive: 9.9221860131898225  \n",
    "restricted_stock: 9.2128106219771002  \n",
    "total_payments: 8.7727777300916756  \n",
    "shared_receipt_with_poi: 8.589420731682381  \n",
    "loan_advances: 7.1840556582887247  \n",
    "expenses: 6.0941733106389453  \n",
    "from_poi_to_this_person: 5.2434497133749582  \n",
    "other: 4.1874775069953749  \n",
    "from_this_person_to_poi: 2.3826121082276739  \n",
    "director_fees: 2.1263278020077054  \n",
    "to_messages: 1.6463411294420076  \n",
    "deferral_payments: 0.22461127473600989  \n",
    "from_messages: 0.16970094762175533  \n",
    "restricted_stock_deferred: 0.065499652909942141\n",
    "\n",
    "###### Feature Creation\n",
    "\n",
    "<p>Additional features 'percent_exercised_stock' and 'percent_bonus' were created but their feature importances were very low.  'percent_exercised_stock' was the percent of 'total_stock_value' that was exercised and 'percent_bonus' was the percent of 'salary' equal to an employee's bonus.</p>\n",
    "\n",
    "exercised_stock_options: 21.153646538437151  \n",
    "total_stock_value: 20.492888346982209  \n",
    "bonus: 17.326074648455403  \n",
    "salary: 14.579307471130718  \n",
    "percent_bonus: 8.5695799793786058  \n",
    "percent_exercised_stock: 0.6724199516795144  \n",
    "\n",
    "<p>The four features with the highest importance scores were selected.  The first element 'poi' is a label.</p>    \n",
    "\n",
    "```Python\n",
    "features_list = ['poi','exercised_stock_options', 'total_stock_value', 'bonus', 'salary']\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting classifiers without training/testing\n",
    "\n",
    "<p>A variety of classifiers were fit to all of the features and labels in the data set.  The features were used to make predictions about labels and these predicted labels were compared with the true labels.  The accuracy, precision, and recall of each algorithm was measured.  The precision is the amount of true positives divided by the sum of true positives and false positives.  It is the number of true positive 'poi' labels divided by all positive 'poi' labels regardless of whether or not they are true.  The recall is the number of true positives divided by the sum of true positives and false negatives.  It is the probability of algorithm to correctly identify a 'poi'.</p>\n",
    "\n",
    "###### GaussianNB\n",
    "\n",
    "accuracy: 0.869230769231  \n",
    "precision: 0.545454545455  \n",
    "recall: 0.333333333333\n",
    "\n",
    "###### Decision Tree\n",
    "\n",
    "accuracy: 1.0  \n",
    "precision: 1.0  \n",
    "recall: 1.0  \n",
    "\n",
    "###### SVC\n",
    "\n",
    "accuracy: 1.0  \n",
    "precision: 1.0  \n",
    "recall: 1.0  \n",
    "\n",
    "###### KNeighbors\n",
    "\n",
    "accuracy: 0.907692307692  \n",
    "precision: 0.8  \n",
    "recall: 0.444444444444  \n",
    "\n",
    "###### AdaBoost\n",
    "\n",
    "accuracy: 1.0  \n",
    "precision: 1.0  \n",
    "recall: 1.0 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting classifiers to training and testing sets\n",
    "\n",
    "<p>SK-Learn's 'train_test_split' function was used to split the data into training and testing sets.  This type of validation is useful to avoid overfitting the classifier and attaining a result that is much higher than the actual accuracy. 30% of the data was selected for the testing set.  The same classifiers were fit to the training features and training labels.  The test features were used to make predictions on the test labels and these predicted labels were compared with the true test labels.</p>\n",
    "\n",
    "###### GaussianNB\n",
    "\n",
    "accuracy: 0.897435897436  \n",
    "precision: 0.0  \n",
    "recall: 0.0\n",
    "\n",
    "###### Decision Tree\n",
    "\n",
    "accuracy: 0.871794871795  \n",
    "precision: 0.333333333333  \n",
    "recall: 0.25  \n",
    "\n",
    "###### SVC\n",
    "\n",
    "accuracy: 0.897435897436  \n",
    "precision: 0.0  \n",
    "recall: 0.0  \n",
    "\n",
    "###### KNeighborsClassifier . \n",
    "\n",
    "accuracy: 0.923076923077    \n",
    "precision: 1.0  \n",
    "recall: 0.25      \n",
    "\n",
    "###### Adaboost\n",
    "\n",
    "accuracy: 0.769230769231  \n",
    "precision: 0.0  \n",
    "recall: 0.0  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameter Tuning with GridSearchCV\n",
    "\n",
    "<p>KNeighborsClassifier was selected for parameter tuning because it had the best performance after training and testing.  Parameter tuning can have a large impact on the final result of an algorithm.  When the default value of 5 for 'n_neighbors' was used the recall was only 0.25.  The 'n_neighbors' and 'weights' parameters were varied.</p>\n",
    "\n",
    "###### KNeighborsClassifier\n",
    "\n",
    "best_parameters: {'n_neighbors': 1, 'weights': 'uniform'}  \n",
    "accuracy: 0.846153846154  \n",
    "precision: 0.333333333333  \n",
    "recall: 0.5  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Effects of Additional Features\n",
    "\n",
    "<p>The KNeighborsClassifier was attempted with the percentage features created earlier, rather than the original features.  These scaled features increased the accuracy but resulted in an undefined precision score.</p>\n",
    "```Python\n",
    "features_list = ['poi','percent_exercised_stock', 'percent_bonus']\n",
    "```\n",
    "\n",
    "###### KNeighborsClassifier\n",
    "\n",
    "best_parameters: {'n_neighbors': 2, 'weights': 'uniform'}  \n",
    "accuracy: 0.916666666667  \n",
    "precision: 0.0  \n",
    "recall: 0.0  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
