{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identify Fraud from Enron Email"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Machine learning techniques were applied to data from 146 Enron employees.  There were 21 features for each employee and 3066 total data points.  The data for each employee fell into 3 categories: financial, email, and poi label.  The poi label signified whether or not an employee was a person of interest in the Enron case.  18 employees were labeled as a poi and 128 employees were not.  5 machine learning algorithms were deployed to predict an employee's poi label based on their financial and email features.  1 algorithm was selected and tuned for final analysis.</p>\n",
    "\n",
    "### Data Structure\n",
    "\n",
    "The data was stored in a dictionary of dictionaries.  For the outer dict, each key was a person and each value was a feature name.  For the inner dict, each key was a feature name and each value was the value of that feature.  \n",
    "\n",
    "```Python\n",
    "datadict['SKILLING JEFFREY K']['salary']\n",
    "```\n",
    "\n",
    "###### Financial Features\n",
    "\n",
    "<p>['salary', 'deferral_payments', 'total_payments', 'loan_advances', 'bonus', 'restricted_stock_deferred', 'deferred_income', 'total_stock_value', 'expenses', 'exercised_stock_options', 'other', 'long_term_incentive', 'restricted_stock', 'director_fees'] (all units are in US dollars)</p>\n",
    "\n",
    "###### Email Features\n",
    "\n",
    "<p>['to_messages', 'email_address', 'from_poi_to_this_person', 'from_messages', 'from_this_person_to_poi', 'shared_receipt_with_poi'] (units are generally number of emails messages; notable exception is ‘email_address’, which is a text string)</p>\n",
    "\n",
    "###### POI Label\n",
    "\n",
    "<p>[‘poi’] (boolean, represented as integer)</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Missing Values\n",
    "\n",
    "<p>These are the total 'NaN' values for each feature.</p>\n",
    "\n",
    "POI: 0  \n",
    "Salary: 50  \n",
    "Deferred Income: 96  \n",
    "Loan Advances: 141  \n",
    "Other: 53  \n",
    "Long Term Incentive: 79  \n",
    "Percent Exercised Stock: 44  \n",
    "Percent Restricted Stock: 38  \n",
    "Percent Restricted Stock Deferred: 130  \n",
    "Percent to POI: 58  \n",
    "Percent from POI: 58  \n",
    "Percent Shared with POI: 58  \n",
    "Percent Deferral Payments: 106  \n",
    "Percent Expenses: 50  \n",
    "Percent Director Fees: 130  \n",
    "Percent Bonus: 63  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outliers\n",
    "\n",
    "<p>Many outliers in this dataset were important because they helped identify persons of interest.  However, some outliers did not correspond to a person.  The max value for financial features was from a 'TOTAL' key rather than a 'PERSON' key.  Another non-person key 'THE TRAVEL AGENCY IN THE PARK' was identified and these keys were removed from the data dictionary.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Feature Selection\n",
    "\n",
    "<p>SelectKBest was used to get the feature importances for all features.  The feature importances are based on the chi squared value between each feature and the poi label.  The four features with the highest importance scores were selected.  The first element 'poi' is a label.  Feature scaling was not required for the decision tree algorithm that was deployed.</p>      \n",
    "\n",
    "```Python\n",
    "features_list = ['poi','exercised_stock_options', 'total_stock_value', 'bonus', 'salary']\n",
    "```  \n",
    "\n",
    "exercised_stock_options: 24.815079733218194  \n",
    "total_stock_value: 24.182898678566879  \n",
    "bonus: 20.792252047181535  \n",
    "salary: 18.289684043404513  \n",
    "deferred_income: 11.458476579280369  \n",
    "long_term_incentive: 9.9221860131898225  \n",
    "restricted_stock: 9.2128106219771002  \n",
    "total_payments: 8.7727777300916756  \n",
    "shared_receipt_with_poi: 8.589420731682381  \n",
    "loan_advances: 7.1840556582887247  \n",
    "expenses: 6.0941733106389453  \n",
    "from_poi_to_this_person: 5.2434497133749582  \n",
    "other: 4.1874775069953749  \n",
    "from_this_person_to_poi: 2.3826121082276739  \n",
    "director_fees: 2.1263278020077054  \n",
    "to_messages: 1.6463411294420076  \n",
    "deferral_payments: 0.22461127473600989  \n",
    "from_messages: 0.16970094762175533  \n",
    "restricted_stock_deferred: 0.065499652909942141\n",
    "\n",
    "### Feature Creation\n",
    "\n",
    "<p>Additional features 'percent_exercised_stock' and 'percent_bonus' were created but their feature importances were very low.  'percent_exercised_stock' was the percent of 'total_stock_value' that was exercised and 'percent_bonus' was the percent of 'salary' equal to an employee's bonus.</p>\n",
    "\n",
    "exercised_stock_options: 21.153646538437151  \n",
    "total_stock_value: 20.492888346982209  \n",
    "bonus: 17.326074648455403  \n",
    "salary: 14.579307471130718  \n",
    "percent_bonus: 8.5695799793786058  \n",
    "percent_exercised_stock: 0.6724199516795144  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting classifiers without validation\n",
    "\n",
    "<p>A variety of classifiers were fit to all of the features and labels in the data set.  The features were used to make predictions about labels and these predicted labels were compared with the true labels.  The accuracy, precision, and recall of each algorithm was measured.  The precision is the amount of true positives divided by the sum of true positives and false positives.  It is the number of true positive 'poi' labels divided by all positive 'poi' labels regardless of whether or not they are true.  The recall is the number of true positives divided by the sum of true positives and false negatives.  It is the probability of algorithm to correctly identify a 'poi'.</p>\n",
    "\n",
    "###### GaussianNB\n",
    "\n",
    "accuracy: 0.869230769231  \n",
    "precision: 0.545454545455  \n",
    "recall: 0.333333333333\n",
    "\n",
    "###### Decision Tree\n",
    "\n",
    "accuracy: 1.0  \n",
    "precision: 1.0  \n",
    "recall: 1.0  \n",
    "\n",
    "###### SVC\n",
    "\n",
    "accuracy: 1.0  \n",
    "precision: 1.0  \n",
    "recall: 1.0  \n",
    "\n",
    "###### KNeighbors\n",
    "\n",
    "accuracy: 0.907692307692  \n",
    "precision: 0.8  \n",
    "recall: 0.444444444444  \n",
    "\n",
    "###### AdaBoost\n",
    "\n",
    "accuracy: 1.0  \n",
    "precision: 1.0  \n",
    "recall: 1.0 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GridSearchCV\n",
    "\n",
    "###### Cross Validation\n",
    "\n",
    "<p>A decision tree algorithm was tuned using GridSearchCV.  Because GridSearchCV can do cross validation, the whole data set was passed to the algorithm instead of splitting the data into testing and training sets.  This is more useful because when data is split into train/test sets, the goal is to maximize the training set size to acheive the best learning outcome as well as maximize the test set size to acheive the best validation.  Partitioning the data like this caused a tradeoff because every data point used for the test set cannot be used by the training set and visa versa.  Cross validation partitions the data into bins of equal size and the learning experiment is run once for every bin in the set.  One bin is selected as the test set and the rest of the bins are selected for training.   This is done for every bin and the test results for each bin are averaged.  This takes more computing time but has better accuracy than training and testing sets.</p>  \n",
    "\n",
    "###### Parameter Tuning\n",
    "\n",
    "<p>Failing to tune the parameters of an algorithm, or tuning them poorly, can lessen the accuracy of the classifier.  Poor tuning can also increase the accuracy of the classifier and precision score while decreasing the recall score.  The 'splitter' parameter was varied between \"best\" and \"random\" and the 'random_state' parameter was varied between 'None, 20, 30, 40, 50, 60'.  The 'max_depth' parameter was initially adjusted as well, but it had poor results.</p>\n",
    "\n",
    "###### Decision Tree\n",
    "\n",
    "best_parameters: {'splitter': \"random\", 'random_state': 40}  \n",
    "accuracy: 0.79592  \n",
    "precision: 0.33367   \n",
    "recall: 0.32750 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Effects of Additional Features\n",
    "\n",
    "<p>The decision tree algorithm was attempted with one of the percentage features created earlier, rather than the original features.  The results were slightly worse.</p>  \n",
    "```Python\n",
    "features_list = ['poi','percent_exercised_stock', 'bonus', 'salary']\n",
    "```\n",
    "\n",
    "###### Decision Tree\n",
    "\n",
    "best_parameters: {'splitter': \"random\", 'random_state': 55}  \n",
    "accuracy: 0.78108  \n",
    "precision: 0.30825    \n",
    "recall: 0.34000    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Improving Feature Selection\n",
    "\n",
    "<p>I tried the decision tree algorithm again with only the top 3 features from the SelectKBest scores earlier.  The results were slightly better.</p>\n",
    "\n",
    "```Python\n",
    "features_list = ['poi','exercised_stock_options', 'total_stock_value', 'bonus']\n",
    "```  \n",
    "###### Decision Tree\n",
    "\n",
    "best_parameters: {'splitter': \"random\", 'random_state': 50}  \n",
    "accuracy: 0.81223  \n",
    "precision: 0.39249    \n",
    "recall: 0.40250    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
